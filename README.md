# ğŸ¤– Test App From this link 
(https://melbornhousepriceprediction-7sjxjfjfcagahqlfz2s9tp.streamlit.app/)


# ğŸ¡ Melbourne House Price Prediction

An end-to-end machine learning project to predict **house prices in Melbourne, Australia** using:
- Cleaned + engineered tabular data  
- Robust preprocessing pipelines  
- Advanced gradient boosting models (CatBoost, XGBoost, LightGBM)  
- Target transformation (log1p / expm1)  
- Hyperparameter tuning  
- A fully exportable production-ready model (`model.pkl`)

---

## ğŸ“Œ Project Overview

The goal of this project is to build a **reliable regression model** that can predict the sale price of houses in Melbourne based on features such as:
- Location (suburb, region, council area, distance from CBD)  
- Property characteristics (rooms, bathrooms, car spots, landsize, year built)  
- Sale information (method of sale, season, seller agency)  

The project is structured to reflect **good real-world ML practices**:
1. Data cleaning & feature engineering  
2. Exploratory data analysis (EDA) with visualizations  
3. Preprocessing using `Pipeline` + `ColumnTransformer`  
4. Model comparison across several algorithms  
5. Use of `TransformedTargetRegressor` for skewed targets  
6. Hyperparameter tuning for the best model  
7. Model persistence using `joblib`  

---

## ğŸ“‚ Project Structure

```bash
.
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ melb_data.csv          # Raw dataset
â”‚   â””â”€â”€ cleaned_data.csv       # Clean dataset after preprocessing
â”‚
â”œâ”€â”€ Data_Cleaning.ipynb          # Data cleaning & feature engineering
â”œâ”€â”€ Pipline.ipynb                 # Preprocessing pipelines & model training
â”œâ”€â”€ model.pkl                  # Final trained model (CatBoost + target transform)
â”œâ”€â”€ requirements.txt           # Project dependencies
â””â”€â”€ README.md                  # Project documentation
```

---
.







## ğŸ§¹ Data Cleaning & Feature Engineering

All cleaning is handled in **`Data_Cleaning.py`**, including:

- Handling missing values  
- Dropping high-missing or low-information columns  
- Parsing the `Date` feature into:
  - `year`, `month`, `day`, `season`
- Cleaning and splitting the `Address` into:
  - `street_name` (later dropped for modeling to avoid high cardinality noise)  
- Ensuring correct data types for numeric and categorical variables  
- Exporting the cleaned dataset to `cleaned_data.csv`

Example of season extraction:

```python
df['season'] = df['month'] % 12 // 3 + 1
```


## ğŸ¤– Models & Training

Several models were evaluated using a unified pipeline:

- Linear Regression  
- KNN  
- Decision Tree  
- Random Forest  
- XGBoost  
- LightGBM  
- **CatBoost (final chosen model)**  



## ğŸ¯ Target Transformation

House prices are **highly skewed**, so a `TransformedTargetRegressor` is used:

- Stabilize variance  
- Reduce the impact of very expensive properties  
- Improve model performance and generalization  

---

## ğŸ” Hyperparameter Tuning (CatBoost)

Hyperparameters tuned with `RandomizedSearchCV`:


## ğŸ† Final Model & Performance

The final chosen model is:

- **CatBoostRegressor with tuned hyperparameters**
- Wrapped in:
  - `Pipeline` (for preprocessing)
  - `TransformedTargetRegressor` (for log-scaling the target)

**Gasser Ahmed**  
_Data Scientist & Machine Learning Enthusiast_
